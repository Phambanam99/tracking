# Storage Optimization - Implementation Summary ðŸŽ¯

## âœ… ÄÃ£ Thá»±c Hiá»‡n

### 1. Schema Changes (`backend/prisma/schema.prisma`)

#### Changed: VesselPosition Unique Constraint
```prisma
// âŒ Before: LÆ°u táº¥t cáº£ sources riÃªng biá»‡t
@@unique([vesselId, timestamp, source])

// âœ… After: Chá»‰ lÆ°u 1 record tá»‘t nháº¥t má»—i timestamp
@@unique([vesselId, timestamp])
```

**Impact:** Giáº£m 50% storage ngay láº­p tá»©c

#### Added: VesselPositionArchive Table
```prisma
model VesselPositionArchive {
  id          Int      @id @default(autoincrement())
  vesselId    Int
  latitude    Float
  longitude   Float
  speed       Float?
  course      Int?
  heading     Int?
  status      String?
  timestamp   DateTime  // Rounded to minute
  source      String?
  score       Float?
  sampleCount Int?      // â† NEW: Sá»‘ samples Ä‘Æ°á»£c aggregate
  avgSpeed    Float?    // â† NEW: Average speed trong minute
  
  vessel Vessel @relation(fields: [vesselId], references: [id], onDelete: Cascade)
  
  @@unique([vesselId, timestamp])
  @@index([vesselId])
  @@index([timestamp])
  @@map("vessel_positions_archive")
}
```

**Purpose:** Long-term storage vá»›i downsampling (1 record/minute thay vÃ¬ /5 seconds)

### 2. Fusion Logic Changes (`backend/src/ais/ais-orchestrator.service.ts`)

#### Changed: processFusion Method
```typescript
// âŒ Before: Persist cáº£ realtime vÃ  backfill
if (decision.publish || decision.backfillOnly) {
  await this.persist(decision.best);
}

// âœ… After: Chá»‰ persist message má»›i (realtime)
if (decision.publish) {
  const fused = this.toFusedRecord(decision.best);
  this.fused$.next(fused);
  this.stats.published++;
  await this.vesselFusion.markPublished(key, decision.best.ts);
  
  // âœ… Only persist best message
  await this.persist(decision.best);
}

// âŒ REMOVED: backfillOnly persistence
```

**Impact:** KhÃ´ng lÆ°u old/duplicate messages vÃ o DB

#### Changed: persist Method
```typescript
// âŒ Before: Unique constraint vá»›i source
where: {
  vesselId_timestamp_source: {
    vesselId: vessel.id,
    timestamp: timestampValue,
    source: sourceValue,
  },
}

// âœ… After: Unique constraint khÃ´ng cÃ³ source
where: {
  vesselId_timestamp: {
    vesselId: vessel.id,
    timestamp: timestampValue,
  },
}
```

**Impact:** Náº¿u cÃ³ 2 messages cÃ¹ng timestamp, message sau (tá»‘t hÆ¡n) sáº½ update message trÆ°á»›c

## ðŸ“Š Storage Comparison

### Before Optimization:
```
Daily Records:
- 28,869 vessels
- Ã— 2 sources (SignalR + AISStream.io)
- Ã— 17,280 updates/day (every 5 seconds)
- = 997,574,400 records/day
- â‰ˆ 100 GB/day
- â‰ˆ 36 TB/year âŒ
```

### After Optimization (Phase 1):
```
Daily Records:
- 28,869 vessels
- Ã— 1 record per timestamp (best message only)
- Ã— 17,280 updates/day
- = 498,787,200 records/day
- â‰ˆ 50 GB/day
- â‰ˆ 18 TB/year âœ… (50% reduction)
```

### After Full Implementation (Phase 1 + 2 + 3):
```
Tier 1 (Redis - 1 hour):     4 GB
Tier 2 (Postgres - 7 days):  70 GB
Tier 3 (Archive - 1 year):   1.4 TB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                        ~1.5 TB âœ… (96% reduction!)
```

## ðŸš€ Next Steps

### Phase 2: Cleanup Jobs (TODO)

Create scheduled jobs for data lifecycle management:

```typescript
// File: backend/src/vessel/vessel-cleanup.service.ts

import { Injectable, Logger } from '@nestjs/common';
import { Cron } from '@nestjs/schedule';
import { PrismaService } from '../prisma/prisma.service';

@Injectable()
export class VesselCleanupService {
  private readonly logger = new Logger(VesselCleanupService.name);

  constructor(private readonly prisma: PrismaService) {}

  /**
   * Daily cleanup: Delete positions older than 7 days
   * Runs at midnight every day
   */
  @Cron('0 0 * * *')
  async cleanupOldPositions() {
    const sevenDaysAgo = new Date(Date.now() - 7 * 24 * 60 * 60 * 1000);
    
    try {
      const result = await this.prisma.vesselPosition.deleteMany({
        where: {
          timestamp: { lt: sevenDaysAgo }
        }
      });
      
      this.logger.log(`Cleaned up ${result.count} old vessel positions`);
    } catch (error) {
      this.logger.error('Failed to cleanup old positions:', error);
    }
  }

  /**
   * Daily archival: Downsample and move to archive
   * Runs at 1 AM every day
   */
  @Cron('0 1 * * *')
  async archiveOldPositions() {
    const sevenDaysAgo = new Date(Date.now() - 7 * 24 * 60 * 60 * 1000);
    const eightDaysAgo = new Date(Date.now() - 8 * 24 * 60 * 60 * 1000);
    
    try {
      // Get positions from 7-8 days ago
      const positions = await this.prisma.vesselPosition.findMany({
        where: {
          timestamp: {
            gte: eightDaysAgo,
            lt: sevenDaysAgo
          }
        },
        orderBy: { timestamp: 'asc' }
      });
      
      // Group by vessel and minute
      const grouped = this.groupByMinute(positions);
      
      // Insert into archive (1 record per minute)
      let archived = 0;
      for (const [key, group] of grouped) {
        const [vesselId, minute] = key.split(':');
        
        await this.prisma.vesselPositionArchive.create({
          data: {
            vesselId: parseInt(vesselId),
            latitude: group[0].latitude,
            longitude: group[0].longitude,
            speed: group[0].speed,
            course: group[0].course,
            heading: group[0].heading,
            status: group[0].status,
            timestamp: new Date(minute),
            source: group[0].source,
            score: group[0].score,
            sampleCount: group.length,
            avgSpeed: this.average(group.map(p => p.speed).filter(s => s !== null)),
          }
        });
        
        archived++;
      }
      
      this.logger.log(`Archived ${archived} downsampled records from ${positions.length} positions`);
    } catch (error) {
      this.logger.error('Failed to archive positions:', error);
    }
  }

  private groupByMinute(positions: any[]) {
    const map = new Map<string, any[]>();
    
    for (const pos of positions) {
      const minute = new Date(pos.timestamp);
      minute.setSeconds(0, 0);
      const key = `${pos.vesselId}:${minute.toISOString()}`;
      
      if (!map.has(key)) map.set(key, []);
      map.get(key)!.push(pos);
    }
    
    return map;
  }

  private average(numbers: number[]): number | null {
    if (numbers.length === 0) return null;
    return numbers.reduce((a, b) => a + b, 0) / numbers.length;
  }
}
```

### Phase 3: Redis TTL (TODO)

Add TTL to Redis keys to auto-expire old data:

```typescript
// In ais-orchestrator.service.ts persist() method

// Redis persistence with TTL
await client.hset(`ais:vessel:${mmsi}`, { ... });
await client.expire(`ais:vessel:${mmsi}`, 3600); // 1 hour TTL

// Geo index with cleanup
await client.geoadd('ais:vessels:geo', msg.lon, msg.lat, mmsi);
// Note: Geo index needs manual cleanup via cron job
```

## ðŸ” Migration Steps

### Step 1: Run Migration
```bash
cd backend
npx prisma migrate dev --name optimize_vessel_position_storage
```

**This will:**
- âœ… Drop old unique constraint `@@unique([vesselId, timestamp, source])`
- âœ… Create new unique constraint `@@unique([vesselId, timestamp])`
- âœ… Create `vessel_positions_archive` table
- âš ï¸ **WARNING:** Existing duplicate records will be deleted (keeps newest)

### Step 2: Restart Backend
```bash
npm run start:dev
```

**Verify:**
- âœ… No errors in logs
- âœ… New positions being saved with correct unique constraint
- âœ… Check database: `SELECT COUNT(*) FROM vessel_positions;`

### Step 3: Monitor Storage
```sql
-- Check table sizes
SELECT 
  pg_size_pretty(pg_total_relation_size('vessel_positions')) as main_size,
  pg_size_pretty(pg_total_relation_size('vessel_positions_archive')) as archive_size;

-- Check record counts
SELECT 
  (SELECT COUNT(*) FROM vessel_positions) as recent_count,
  (SELECT COUNT(*) FROM vessel_positions_archive) as archive_count;

-- Check oldest record
SELECT MIN(timestamp) FROM vessel_positions;
```

## âš ï¸ Important Notes

### Data Loss Warning:
When running the migration, **duplicate records will be removed**. Only the newest record for each `(vesselId, timestamp)` combination will be kept.

**Example:**
```sql
-- Before migration:
vesselId=922767, timestamp='2025-11-08 10:00:00', source='signalr'
vesselId=922767, timestamp='2025-11-08 10:00:00', source='aisstream.io'

-- After migration (keeps newest):
vesselId=922767, timestamp='2025-11-08 10:00:00', source='aisstream.io'
```

### Backup Recommendation:
```bash
# Backup before migration
pg_dump -h localhost -U postgres -d tracking > backup_before_optimization.sql

# Or just backup vessel_positions table
pg_dump -h localhost -U postgres -d tracking -t vessel_positions > vessel_positions_backup.sql
```

## ðŸ“ˆ Expected Results

### Immediate (After Phase 1):
- âœ… 50% storage reduction
- âœ… Faster queries (smaller table)
- âœ… No functionality loss (still have best data)

### After Phase 2 (Cleanup Jobs):
- âœ… 7-day retention in main table
- âœ… Consistent storage size (~70 GB)
- âœ… Automatic archival to downsampled table

### After Phase 3 (Redis TTL):
- âœ… Redis memory under control (~4 GB)
- âœ… No manual Redis cleanup needed

## ðŸŽ¯ Success Metrics

Monitor these metrics after deployment:

1. **Storage Growth Rate:**
   - Before: ~100 GB/day
   - After: ~10 GB/day (with 7-day retention)

2. **Query Performance:**
   - Measure average query time for `/vessels/online`
   - Should improve due to smaller table

3. **Data Quality:**
   - Verify `source` and `score` are not null
   - Verify fusion is selecting best messages

4. **System Health:**
   - No increase in errors
   - Redis memory stable
   - Database CPU/IO stable

---

**Status:** âœ… Phase 1 Complete - Ready for Migration  
**Next:** Run migration and deploy to production  
**Timeline:** 1-2 hours for migration + testing

